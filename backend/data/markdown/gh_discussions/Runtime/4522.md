# MegaBoom, clock trees, macro placement and global routing congestion

Tool: Clock Tree Synthesis

Subcategory: Clock tree and timing analysis

## Conversation

### oharboe
How can I tell if clock skew is increasing the minimum clock period for a design?

Here is my current understanding:

If two flip flops are not connected, then the clock skew between the clocks that drive those two flip flops doesn't matter because there is no timing path between these two flip flops.

Skew can be good and it can be bad. If there is a long timing path between two flip flops, then a negative skew for the starting flip flops or postivive skew for the capturing flip flop would make it easier to meet timing.

As a first order approximation though, the CTS will try to minimize clock skew, because in the end a very large clock skew will catch up with you and increase the minimum clock period.

Latest MegaBoom update:

I have [modified MegaBoom](https://github.com/The-OpenROAD-Project/megaboom/pull/18) so that it no longer has a PLL, but a clock for the TileLink (top level memory/peripheral interface) and for the RISC-V core.

As I understand, though I don't know the code very well, the RISC-V core is connected to the TileLink via an asynchronous FIFO(or equivalent thereof).

Therefore there are no ChipTop inputs/outputs that have an insertion point relative to the clock for the RISC-V core. This seems like a clever way of doing things, because then the insertion latency of the RISC-V clock doesn't matter(though clock uncertainty which I would expect to grow with a long clock insertation latency) for the clock period.

```
>>> report_clock_skew
Clock clock_uncore
Latency      CRPR       Skew
system/tile_prci_domain/tile_reset_domain_boom_tile/lsu/_728430_/CLK ^
 882.50
system/tile_prci_domain/tile_reset_domain_boom_tile/core/int_issue_unit/slots_32/_3607_/CLK ^
1008.42      0.00    -125.92

Clock serial_tl_0_clock
Latency      CRPR       Skew
system/serial_tl_domain/_1154_/CLK ^
  67.76
system/serial_tl_domain/_1275_/CLK ^
  64.77      0.00       2.99
```

![image](https://github.com/The-OpenROAD-Project/OpenROAD/assets/2798822/1df082cd-b853-44f7-8b5e-7ebeb9e1409b)

![image](https://github.com/The-OpenROAD-Project/OpenROAD/assets/2798822/e1288f3f-c699-4fa5-9148-40a2b85e5d43)

Some notes:

- The area of some of those macros are mocked to be smaller than they really are so as to fit this into 1000um x 1000um and have reasonable turnaround times in builds. The L2 is *tiny* in area...
- The longest timing path is part of the top level design(macros are not involved), so nothing material should be lost by ignoring what is inside the macros for now. The macros are abstracts from floorplan, so completely unrealistic.
- A quick test the other day put a flattened design at ca. 3mm^2 and 3 million instances(big caveat, as I write this from my flawed memory, I'm not currently studying a flattened design). To be revisited later maybe.
- W.r.t. the global placement congestion, macro placement has a fix coming up, so I don't think there's anything interesting to study w.r.t. macro placement and global routing congestion until a new build has been done after that fix. https://github.com/The-OpenROAD-Project/OpenROAD/pull/4519
- almost no hold cells, 20, there used to be thousands or even tens of thousands. . :exploding_head: 
- running time for CTS is now much more reasonable, 5000s down from 30000s. This is not entirely suprising: now that the clock tree is not pathologically formed, the repair job is probably quick.


```
Log                       Elapsed seconds
1_0_mem                          173
1_1_yosys                       3606
1_1_yosys_hier_report           3542
2_1_floorplan                    277
2_2_floorplan_io                   8
No elapsed time found in bazel-bin/logs/asap7/ChipTop/base/2_3_floorplan_tdms.log
2_4_floorplan_macro              496
2_5_floorplan_tapcell            130
2_6_floorplan_pdn                206
3_1_place_gp_skip_io             448
3_2_place_iop                     14
3_3_place_gp                    4564
3_4_place_resized                863
3_5_place_dp                    1033
4_1_cts                         5069
5_1_grt                        14525
Total                          34954
```


### rovinski
> If two flip flops are not connected, then the clock skew between the clocks that drive those two flip flops doesn't matter because there is no timing path between these two flip flops.

Correct

> Skew can be good and it can be bad. If there is a long timing path between two flip flops, then a negative skew for the starting flip flops or postivive skew for the capturing flip flop would make it easier to meet timing.

Correct

> As a first order approximation though, the CTS will try to minimize clock skew, because in the end a very large clock skew will catch up with you and increase the minimum clock period.

The OR CTS engine tries to minimize skew because it is algorithmically simpler, but not necessarily the most optimal. Commercial engines use a technique called "concurrent clock optimization" which will look at the timing paths and purposefully skew certain registers if it makes timing better.

Concurrent clock optimization has a similar effect to register retiming - the former shifts the clock so it borrows setup time from one stage to give to another stage. The latter shifts logic from one stage to another stage and therefore also shifts setup time.

> As I understand, though I don't know the code very well, the RISC-V core is connected to the TileLink via an asynchronous FIFO(or equivalent thereof).

Yes, async FIFOs or other clock domain crossings (CDCs) are convenient ways to break up and decouple clock trees. Clock trees cannot become too large, because the larger they are, the more power they consume and the more difficult it is to minimize skew/jitter/uncertainty. A clock tree can become so large that the jitter becomes larger than the clock period, in which case timing is impossible to meet. There is a design tradeoff between how many clock domains there are and data latency because the CDCs add one or more cycles when transmitting data across the interface.

> How can I tell if clock skew is increasing the minimum clock period for a design?

I might rephrase the question more simply as "How can I tell if clock skew is bad for a design?" because clock skew always impacts the clock period, as alluded above. This is one of the areas where it takes a lot of intuition, experimentation, and heuristics to evaluate because the answer is rarely clear. A soft and perhaps unuseful rule of thumb would be "when the clock skew/jitter/uncertainty becomes a significant fraction of the clock period". There are no hard rules of thumb, because sometimes high skew can be tolerated in order to keep the design fully synchronous. I personally start to get suspicious if the skew is eating more than 20-40% of the clock period. But the size of the clock tree also matters and how much skew you would _expect_ from a clock tree of that size.

There are some red flags, though, to identify purely suboptimal results. One is if there are many, many hold buffers being inserted. This is usually due to bad timing constraints, but it could also be due to bad skew in the clock tree.

Another red flag is if a path is failing both setup time _and_ hold time. This most often happens not because of skew but because of jitter caused by on-chip variation. Jitter can cause clock edges to be both early and late, which means that if a path is failing both then the jitter is too high.

### oharboe
This is the [asynchronous connection](https://github.com/The-OpenROAD-Project/megaboom/blob/9d9d44d90545dd747f38f23d1f4c37045edb5d57/rtl/ClockSinkDomain_2.sv#L163C3-L163C3) between TileLink and the rest of the system is in the Verilog code.

Here is [the expected gray counter](https://github.com/The-OpenROAD-Project/megaboom/blob/9d9d44d90545dd747f38f23d1f4c37045edb5d57/rtl/AsyncQueueSink_3.sv#L56C14-L56C34)  and the corresponding [Chisel code](https://github.com/chipsalliance/rocket-chip/blob/50adbdb3e4e18c2b3de57693323f4174b60f9767/src/main/scala/util/AsyncQueue.scala#L145). 



### oharboe
I chased down the synchronous reset although it doesn't show up in the most critical path at the ChipTop level.

For now, I have created a macro out of the BranchPredictor to rein in build times. In that macro the synchronous reset has a very large fanout, which obviously is a disaster for timing.

After some investigation at the top level, I have found out that MegaBoom, [as documented](https://docs.boom-core.org/en/latest/sections/physical-realization.html#register-retiming), is relying heavily on register retiming and that the synchronous reset [is in fact pipelined](https://github.com/The-OpenROAD-Project/megaboom/blob/9d9d44d90545dd747f38f23d1f4c37045edb5d57/rtl/AsyncResetSynchronizerPrimitiveShiftReg_d3_i0.sv#L40).

However, since the design is hierarchical and not flattened, the design won't be able to take advantage of these three pipeline stages. Also, yosys does not support retiming.

Retiming in OpenROAD/yosys has been discussed in some detail [previously](https://github.com/The-OpenROAD-Project/OpenROAD-flow-scripts/discussions/1710#discussioncomment-7882745), I wanted to share the results of my investigation into synchronous reset specifically for MegaBoom.

![image](https://github.com/The-OpenROAD-Project/OpenROAD/assets/2798822/205fd24f-9f6f-4153-ba6e-75a32f1f87a9)


### oharboe
For my part, the questions were answered so closing.

