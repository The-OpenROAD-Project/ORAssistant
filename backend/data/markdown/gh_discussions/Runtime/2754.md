# How can I find out where the routing congestion is?

Tool: Global Routing

Subcategory: Routing congestion

## Conversation

### oharboe
Any tips?

I'm trying a larger version & different version of https://github.com/The-OpenROAD-Project/OpenROAD-flow-scripts/pull/743 and it fails in global routing.

```
[INFO GRT-0101] Running extra iterations to remove overflow.
[WARNING GRT-0227] Reached 20 congestion iterations with less than 15% of reduction between iterations.
[INFO GRT-0197] Via related to pin nodes: 1126254
[INFO GRT-0198] Via related Steiner nodes: 39
[INFO GRT-0199] Via filling finished.
[INFO GRT-0111] Final number of vias: 2236520
[INFO GRT-0112] Final usage 3D: 15578444
[ERROR GRT-0118] Routing congestion too high. Check the congestion heatmap in the GUI.
Error: global_route.tcl, 19 GRT-0118
```

The most recent .odb file to be written is 4_cts.odb, but when I load it and look at the routing congestion map, I don't see anything suspicious. Such arrays have a lot of routing to do, but the routing is simple conceptually: point to point between the array elements.

I'm learning a few tricks with OpenROAD-flow-scripts:

1. read documentation, if you can find it.
2. read the OpenROAD-flow-scripts/flow/*.tcl script for the step. It is normally quite short. From here you can find out what reports are being saved and also what to search for in other examples.


Looking at congestion.rpt, it is full of these. There's nothing in that area, just filler cells. Sometimes overflow is as high as 20.

```
violation type: Horizontal congestion
	srcs: 
	congestion information: capacity:16 usage:17 overflow:1
	bbox = ( 1544.4, 45.9 ) - ( 1544.94, 46.44) on Layer -
```



### maliberty
The violations are related to routing congestion so they can occur over fill cells.  Do you not see congested areas in the heatmap?  I'm not sure what "anything suspicious" means.

Also you could play with the layer derates in grt.  Reducing them makes it more likely you'll get past grt but also more likely you'll have trouble in drt.  We set them more conservatively in the PDK.

### maliberty
Something doesn't make sense.  It fails to route due to congestion but you don't see any in the congestion map.  If you load the congestion report in the drc viewer you should be able to see where the congestion is.

### maliberty
Can you show a local image of the area?  If you click the underlined heat map you'll get an options dialog.  You can investigate on what layers/directions the congestion is happening in.

### oharboe
I'm going to investigate something else... Could it be that the PDN is overlaying the pins at the top of the macro?

Here the blue line (from PDN) is not overlaying the pins at the top of the macro.

![image](https://user-images.githubusercontent.com/2798822/212607947-ac115c6b-dec6-47e2-87f1-17f8da092f63.png)

The vertical PDN lines go all the way to the top, but the horizontal not all the way to the right edge. That's an assymetry that I find puzzling.

The screenshot below is the upper right corner of the design I'm tinkering with:

![image](https://user-images.githubusercontent.com/2798822/212608610-bb1d7d24-7eb3-4d7f-9a1d-6815b7689330.png)

The routing congestion heat map shows that there's a lot of routing at the top (orange). There's nothing there...

![image](https://user-images.githubusercontent.com/2798822/212611152-9861946b-337e-43cb-9153-671785c873a8.png)



### oharboe
I scaled down the design (fewer wires) and then I can see gui_route.

Although there's only horizontal routes between the inner macro's right edge and the top level design's right edge, there's this spagetti just before the input/outputs on the right side...

No wonder this doesn't work if you scale up the number of wires.

I wonder what is going on here...


![image](https://user-images.githubusercontent.com/2798822/212673682-743d7cf8-fe27-435a-9982-f709a7e6ef42.png)

zoom in:


![image](https://user-images.githubusercontent.com/2798822/212674518-062cec9c-ab11-4eeb-92d1-2902cbd6419a.png)




```
set sdc_version 2.0
create_clock [get_ports clock] -period 4000 -waveform {0 2000}

set clk_name  clock
set clk_port_name clock
set clk_period 250
set clk_io_pct 0.2

set clk_port [get_ports $clk_port_name]

set non_clock_inputs [lsearch -inline -all -not -exact [all_inputs] $clk_port]

set_input_delay  [expr $clk_period * $clk_io_pct] -clock $clk_name $non_clock_inputs
set_output_delay [expr $clk_period * $clk_io_pct] -clock $clk_name [all_outputs]

```

### oharboe
@rovinski @maliberty  I've polished up a test case https://github.com/The-OpenROAD-Project/OpenROAD-flow-scripts/pull/764. 

This array is open source and big enough to exhibit the problems, though less pronounced, seen above that will appear as you go from from mock-array-big to a bigger version, yet it will build in a more reasonable amount of time.

In this pull request, I increase the datapath from 8 to 64 bits, at which point global routing fails: https://github.com/The-OpenROAD-Project/OpenROAD-flow-scripts/pull/774




### maliberty
I looked at https://github.com/The-OpenROAD-Project/OpenROAD-flow-scripts/pull/764.  I see a path like:
![image](https://user-images.githubusercontent.com/761514/212960378-5042926e-b92f-4ea5-8493-1424c25f7294.png)

The input arrives at 50ps (20% of your 250ps clock period).   The capture clock path is at nearly 200ps with 44ps of hold from the liberty:
```
>>> report_checks -through hold2693/A -path_delay min -format full_clock_expanded
Startpoint: io_insVertical_0_2[4] (input port clocked by clock)
Endpoint: ces_2_7 (rising edge-triggered flip-flop clocked by clock)
Path Group: clock
Path Type: min

  Delay    Time   Description
---------------------------------------------------------
   0.00    0.00   clock clock (rise edge)
   0.00    0.00   clock network delay (propagated)
  50.00   50.00 ^ input external delay
   0.00   50.00 ^ io_insVertical_0_2[4] (in)
  10.10   60.10 ^ hold3918/Y (BUFx2_ASAP7_75t_R)
  11.76   71.86 ^ hold2691/Y (BUFx2_ASAP7_75t_R)
  11.77   83.63 ^ hold3919/Y (BUFx2_ASAP7_75t_R)
  11.61   95.24 ^ hold1087/Y (BUFx2_ASAP7_75t_R)
  11.64  106.88 ^ hold2692/Y (BUFx2_ASAP7_75t_R)
  11.81  118.69 ^ hold245/Y (BUFx2_ASAP7_75t_R)
  11.74  130.43 ^ hold2693/Y (BUFx2_ASAP7_75t_R)
  11.65  142.08 ^ hold1088/Y (BUFx2_ASAP7_75t_R)
  11.75  153.83 ^ hold2694/Y (BUFx2_ASAP7_75t_R)
  11.95  165.78 ^ input149/Y (BUFx2_ASAP7_75t_R)
  12.04  177.82 ^ hold2695/Y (BUFx2_ASAP7_75t_R)
  12.02  189.84 ^ hold1089/Y (BUFx2_ASAP7_75t_R)
  12.10  201.94 ^ hold2696/Y (BUFx2_ASAP7_75t_R)
  12.11  214.05 ^ hold246/Y (BUFx2_ASAP7_75t_R)
  12.07  226.12 ^ hold2697/Y (BUFx2_ASAP7_75t_R)
  11.89  238.01 ^ hold1090/Y (BUFx2_ASAP7_75t_R)
  12.17  250.19 ^ hold2698/Y (BUFx2_ASAP7_75t_R)
   0.05  250.24 ^ ces_2_7/io_ins_1[4] (Element)
         250.24   data arrival time

   0.00    0.00   clock clock (rise edge)
   0.00    0.00   clock source latency
   0.00    0.00 ^ clock (in)
  16.72   16.72 ^ wire1/Y (BUFx12f_ASAP7_75t_R)
  74.94   91.66 ^ clkbuf_0_clock/Y (BUFx4_ASAP7_75t_R)
  51.85  143.51 ^ clkbuf_2_1_0_clock/Y (BUFx4_ASAP7_75t_R)
  44.16  187.67 ^ clkbuf_3_3__f_clock/Y (BUFx4_ASAP7_75t_R)
   8.97  196.64 ^ ces_2_7/clock (Element)
   0.00  196.64   clock reconvergence pessimism
  44.22  240.86   library hold time
         240.86   data required time
---------------------------------------------------------
         240.86   data required time
        -250.24   data arrival time
---------------------------------------------------------
           9.39   slack (MET)
```

So you'll need 200ps of hold buffering to make this pass.  BUFx2 is ~12ps so you need a good number to meet your hold constraint.  I don't see anything wrong here other than inputs that arrive quite early relative to the macro's requirements.

The clock path doesn't look unreasonable:
![image](https://user-images.githubusercontent.com/761514/212962157-a2e3bc6e-911d-4243-a648-39f7e10343d2.png)


### maliberty
The most forgiving would be to put no i/o timings at all aside from clock.  Then only paths internal to the block would be checked.  Another option would be to give different min/max timings where they are both forgiving.

